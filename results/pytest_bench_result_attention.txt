---------------------------------------------------------------------------------------------------------------- benchmark: 54 tests -----------------------------------------------------------------------------------------------------------------
Name (time in us)                                                    Min                    Max                   Mean                StdDev                 Median                 IQR            Outliers          OPS            Rounds  Iterations
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[256-8192]     12.6620 (1.0)      13,894.3040 (369.65)   11,499.8841 (834.07)   4,619.9281 (>1000.0)  13,328.0490 (988.73)   246.2600 (876.33)  1022;1034      86.9574 (0.00)       7346           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[512-8192]     12.6630 (1.00)     11,337.1130 (301.62)    9,886.8849 (717.08)   3,665.3038 (>1000.0)  11,244.2540 (834.14)    18.7877 (66.86)   1022;1315     101.1441 (0.00)       8439           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[256-256]      12.6930 (1.00)         76.4550 (2.03)         57.2331 (4.15)        13.6793 (7.78)         61.4900 (4.56)       0.2810 (1.0)     1336;1662  17,472.4011 (0.24)      14769           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[256-1024]     12.6960 (1.00)        259.6900 (6.91)        216.8516 (15.73)       72.8151 (41.39)       243.2140 (18.04)      4.5625 (16.24)   1084;1393   4,611.4478 (0.06)       9544           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[64-256]       12.7130 (1.00)         37.5880 (1.0)          15.4684 (1.12)         1.7591 (1.0)          16.6190 (1.23)       3.3290 (11.85)     4584;35  64,647.9031 (0.89)      13256           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[512-4096]     12.7300 (1.01)      3,558.6520 (94.68)     3,089.8918 (224.11)   1,087.1826 (618.02)    3,476.5680 (257.91)    26.7050 (95.03)   1025;1028     323.6359 (0.00)       9235           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[256-4096]     12.7400 (1.01)      3,785.1780 (100.70)    3,047.1778 (221.01)   1,061.4220 (603.38)    3,412.4195 (253.15)    48.4840 (172.53)  1025;1055     328.1725 (0.00)       9406           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[64-4096]      12.7440 (1.01)        826.3980 (21.99)       600.1474 (43.53)      212.0047 (120.52)      676.4705 (50.18)      5.4030 (19.23)   1046;1077   1,666.2574 (0.02)       9002           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[512-256]      12.7690 (1.01)        108.5690 (2.89)         74.3628 (5.39)        21.3076 (12.11)        81.7450 (6.06)       0.4640 (1.65)    1235;1415  13,447.5917 (0.19)      11249           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[64-8192]      12.7870 (1.01)      2,325.2680 (61.86)     2,055.5185 (149.08)     706.1734 (401.43)    2,299.1695 (170.56)     4.4360 (15.79)   1027;1093     486.4953 (0.01)       9616           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[256-512]      12.8140 (1.01)        134.0460 (3.57)         95.3264 (6.91)        27.7783 (15.79)       104.7520 (7.77)       0.5740 (2.04)    1184;1402  10,490.2712 (0.14)      11333           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[512-1024]     12.8240 (1.01)        318.7910 (8.48)        269.0953 (19.52)       81.2372 (46.18)       294.7260 (21.86)      2.0917 (7.44)    1071;1151   3,716.1552 (0.05)      11679           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[64-1024]      12.8250 (1.01)        133.1190 (3.54)         50.5922 (3.67)        13.1033 (7.45)         55.3610 (4.11)       0.2910 (1.04)    1375;1908  19,765.9031 (0.27)      11909           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[256-128]      12.8550 (1.02)         57.2790 (1.52)         13.7876 (1.0)          1.8841 (1.07)         13.4800 (1.0)        0.4970 (1.77)        36;62  72,528.6954 (1.0)         794           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[512-128]      12.8620 (1.02)         51.8730 (1.38)         14.1888 (1.03)         2.8692 (1.63)         13.5870 (1.01)       0.4765 (1.70)        48;75  70,477.9693 (0.97)        853           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[64-512]       12.8800 (1.02)         46.7930 (1.24)         25.9975 (1.89)         5.2323 (2.97)         28.1460 (2.09)       0.3030 (1.08)    2051;2266  38,465.1729 (0.53)      13334           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[512-512]      12.8950 (1.02)        151.6250 (4.03)        121.5275 (8.81)        32.8812 (18.69)       131.5230 (9.76)       0.3788 (1.35)    1143;1378   8,228.5882 (0.11)      13439           1
test_flash_attention_fp8_e5m2_acc_fp32_feather_gpu[64-128]       17.6660 (1.40)         90.4740 (2.41)         35.7472 (2.59)        30.9066 (17.57)        21.2490 (1.58)      25.1943 (89.66)         1;1  27,974.2199 (0.39)          5           1
test_attention_fp32_torch[64-1024]                               42.0680 (3.32)        275.3970 (7.33)        252.4253 (18.31)       43.6031 (24.79)       261.1740 (19.37)      1.6225 (5.77)      137;530   3,961.5674 (0.05)       3243           1
test_attention_fp32_torch[256-256]                               42.5430 (3.36)        137.8610 (3.67)         46.8087 (3.39)         3.4574 (1.97)         47.4130 (3.52)       3.7792 (13.45)     949;154  21,363.5721 (0.29)       5091           1
test_attention_fp32_torch[512-128]                               42.8470 (3.38)         83.3200 (2.22)         44.1612 (3.20)         2.2841 (1.30)         43.5070 (3.23)       0.5333 (1.90)      248;418  22,644.3084 (0.31)       3041           1
test_attention_fp32_torch[256-128]                               43.1300 (3.41)         87.5430 (2.33)         44.7187 (3.24)         2.8127 (1.60)         44.0070 (3.26)       0.5190 (1.85)      158;351  22,362.0139 (0.31)       2675           1
test_attention_fp32_torch[512-256]                               43.2720 (3.42)        123.1500 (3.28)         88.7943 (6.44)         9.8709 (5.61)         90.7880 (6.74)       1.1995 (4.27)      233;534  11,261.9892 (0.16)       4125           1
test_attention_fp32_torch[512-512]                               43.6680 (3.45)        213.3300 (5.68)        191.4394 (13.88)       35.3643 (20.10)       200.1070 (14.84)      1.6995 (6.05)      122;308   5,223.5860 (0.07)       2191           1
test_attention_fp32_torch[256-1024]                              44.1520 (3.49)        466.7690 (12.42)       428.5981 (31.09)       69.6325 (39.58)       441.2590 (32.73)      1.9235 (6.84)      104;265   2,333.1882 (0.03)       3225           1
test_attention_fp32_torch[512-1024]                              44.6960 (3.53)        776.8170 (20.67)       702.0815 (50.92)      112.5744 (63.99)       721.1770 (53.50)      4.5675 (16.25)      99;230   1,424.3361 (0.02)       3453           1
test_attention_fp32_torch[256-512]                               44.9220 (3.55)        178.8330 (4.76)        148.7459 (10.79)       23.1476 (13.16)       153.6530 (11.40)      1.5925 (5.67)      138;402   6,722.8723 (0.09)       2680           1
test_attention_fp32_torch[512-4096]                              45.1500 (3.57)      9,459.8050 (251.67)    8,966.1074 (650.30)   1,520.8366 (864.54)    9,239.9480 (685.46)    60.9815 (217.01)    102;111     111.5311 (0.00)       3608           1
test_attention_fp32_torch[64-4096]                               45.1560 (3.57)      4,224.3450 (112.39)    3,707.1612 (268.88)   1,254.6899 (713.24)    4,124.6240 (305.98)    67.1070 (238.80)    103;103     269.7482 (0.00)        973           1
test_attention_fp32_torch[256-4096]                              45.3920 (3.58)      5,833.9910 (155.21)    5,559.6452 (403.23)     901.4073 (512.42)    5,706.0580 (423.30)    30.4300 (108.29)    103;130     179.8676 (0.00)       3922           1
test_attention_fp32_torch[64-512]                                45.5750 (3.60)        141.9190 (3.78)         84.9561 (6.16)        11.8624 (6.74)         89.6145 (6.65)       3.1800 (11.32)     240;618  11,770.7807 (0.16)       2598           1
test_attention_fp32_torch[64-256]                                45.8810 (3.62)        284.0090 (7.56)         50.0771 (3.63)        19.3621 (11.01)        47.7450 (3.54)       1.8280 (6.51)         2;14  19,969.2162 (0.28)        152           1
test_attention_fp32_torch[64-128]                                47.3190 (3.74)        199.6820 (5.31)         64.8899 (4.71)        42.9412 (24.41)        50.9640 (3.78)       6.0260 (21.44)         1;2  15,410.7152 (0.21)         12           1
test_attention_fp32_torch[64-8192]                               48.3600 (3.82)     16,757.0850 (445.81)   15,603.8044 (>1000.0)  3,861.9482 (>1000.0)  16,559.7970 (>1000.0)   80.5810 (286.75)    102;111      64.0869 (0.00)       1747           1
test_attention_fp32_torch[512-8192]                              48.5200 (3.83)     35,398.3380 (941.75)   33,289.9580 (>1000.0)  6,634.7691 (>1000.0)  34,633.7225 (>1000.0)  844.1830 (>1000.0)   102;102      30.0391 (0.00)       2654           1
test_attention_fp32_torch[256-8192]                              48.8750 (3.86)     23,326.4680 (620.58)   22,128.8437 (>1000.0)  3,836.4037 (>1000.0)  22,739.9370 (>1000.0)  417.8057 (>1000.0)   102;102      45.1899 (0.00)       3467           1
test_attention_fp16_torch[64-1024]                               58.6430 (4.63)        503.2760 (13.39)       307.5165 (22.30)       43.6272 (24.80)       316.1815 (23.46)      1.7240 (6.13)      137;417   3,251.8580 (0.04)       2512           1
test_attention_fp16_torch[256-256]                               59.1810 (4.67)        155.7380 (4.14)         62.6846 (4.55)         3.0467 (1.73)         62.7265 (4.65)       3.1205 (11.10)      275;85  15,952.8820 (0.22)       4588           1
test_attention_fp16_torch[64-128]                                59.3650 (4.69)         90.8820 (2.42)         60.8724 (4.41)         2.7454 (1.56)         60.3080 (4.47)       0.6220 (2.21)        17;34  16,427.8198 (0.23)        299           1
test_attention_fp16_torch[64-256]                                59.5160 (4.70)         87.8740 (2.34)         61.2265 (4.44)         2.2770 (1.29)         60.5960 (4.50)       0.6502 (2.31)      588;938  16,332.7963 (0.23)       7001           1
test_attention_fp16_torch[512-128]                               59.5350 (4.70)        104.1910 (2.77)         61.4280 (4.46)         2.7539 (1.57)         60.6080 (4.50)       0.8583 (3.05)      202;347  16,279.2208 (0.22)       2583           1
test_attention_fp16_torch[256-128]                               59.5360 (4.70)        105.1310 (2.80)         61.2428 (4.44)         2.4922 (1.42)         60.5750 (4.49)       0.6705 (2.39)      237;424  16,328.4589 (0.23)       3260           1
test_attention_fp16_torch[64-512]                                60.3230 (4.76)        109.9460 (2.93)         93.6623 (6.79)         6.6733 (3.79)         95.0180 (7.05)       0.9660 (3.44)      218;311  10,676.6566 (0.15)       4545           1
test_attention_fp16_torch[512-256]                               61.0190 (4.82)        167.7550 (4.46)        120.3358 (8.73)        10.8433 (6.16)        122.0890 (9.06)       1.3393 (4.77)      209;573   8,310.0820 (0.11)       4361           1
test_attention_fp16_torch[256-1024]                              61.7360 (4.88)        573.5140 (15.26)       511.0266 (37.06)       72.8801 (41.43)       522.7780 (38.78)      2.0290 (7.22)       73;175   1,956.8454 (0.03)       2783           1
test_attention_fp16_torch[64-4096]                               61.8860 (4.89)      5,009.8210 (133.28)    4,737.9757 (343.64)     706.8989 (401.84)    4,834.7025 (358.66)    64.0370 (227.88)      69;70     211.0606 (0.00)       3050           1
test_attention_fp16_torch[512-1024]                              62.0750 (4.90)        865.1640 (23.02)       813.3367 (58.99)      101.8718 (57.91)       827.1080 (61.36)      3.4130 (12.15)      69;132   1,229.5030 (0.02)       3777           1
test_attention_fp16_torch[512-512]                               62.0880 (4.90)        266.1280 (7.08)        236.7567 (17.17)       32.5417 (18.50)       242.6090 (18.00)      1.5642 (5.57)       88;286   4,223.7452 (0.06)       2487           1
test_attention_fp16_torch[256-512]                               63.2950 (5.00)        243.7860 (6.49)        185.6008 (13.46)       17.1850 (9.77)        187.9345 (13.94)      1.4390 (5.12)      119;341   5,387.9083 (0.07)       4930           1
test_attention_fp16_torch[256-4096]                              63.6350 (5.03)      6,517.9620 (173.41)    6,222.7189 (451.33)     837.7930 (476.25)    6,336.7310 (470.08)    10.7605 (38.29)       68;94     160.7015 (0.00)       3741           1
test_attention_fp16_torch[64-8192]                               66.3850 (5.24)     19,413.0560 (516.47)   18,826.9504 (>1000.0)  2,613.8038 (>1000.0)  19,189.1465 (>1000.0)   75.7270 (269.48)      68;94      53.1153 (0.00)       3534           1
test_attention_fp16_torch[512-8192]                              66.5700 (5.26)     37,399.0900 (994.97)   35,965.0842 (>1000.0)  5,347.5903 (>1000.0)  36,806.4810 (>1000.0)  603.0855 (>1000.0)     68;69      27.8047 (0.00)       3108           1
test_attention_fp16_torch[512-4096]                              67.2010 (5.31)     10,178.3810 (270.79)    9,723.5041 (705.23)   1,351.5423 (768.30)    9,918.3535 (735.78)    17.6240 (62.72)      68;187     102.8436 (0.00)       3524           1
test_attention_fp16_torch[256-8192]                              93.3120 (7.37)     25,205.3480 (670.57)   24,361.3215 (>1000.0)  3,750.8417 (>1000.0)  24,955.3690 (>1000.0)  227.5302 (809.68)      68;68      41.0487 (0.00)       2887           1
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------