----------------------------------------------------------------------------------------------------------- benchmark: 30 tests ------------------------------------------------------------------------------------------------------------
Name (time in us)                                              Min                   Max                  Mean                StdDev                Median                IQR            Outliers          OPS            Rounds  Iterations
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
test_attention_fp8_e5m2_acc_fp32_feather_gpu[64-256]       10.4950 (1.0)        246.2350 (8.53)        11.4755 (1.0)          3.8145 (3.24)        11.1800 (1.0)       0.3530 (1.29)      108;459  87,142.3846 (1.0)       10939           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[512-4096]     10.5150 (1.00)     7,690.5640 (266.40)   6,817.1882 (594.07)   2,340.7093 (>1000.0)  7,622.0350 (681.76)    2.5290 (9.24)    1023;1286     146.6880 (0.00)       9666           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[64-512]       10.5790 (1.01)        40.2220 (1.39)        26.7308 (2.33)         4.4887 (3.81)        28.0070 (2.51)      0.3950 (1.44)    1790;2062  37,410.0697 (0.43)      22223           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[128-256]      10.5810 (1.01)        28.8680 (1.0)         14.7210 (1.28)         1.7215 (1.46)        15.4420 (1.38)      0.4390 (1.60)    3917;4055  67,930.3741 (0.78)      20670           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[512-128]      10.6050 (1.01)        45.0240 (1.56)        11.7883 (1.03)         1.7440 (1.48)        11.3990 (1.02)      0.4510 (1.65)        66;82  84,829.5916 (0.97)        970           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[128-512]      10.6580 (1.02)        63.1200 (2.19)        39.0453 (3.40)         8.3846 (7.12)        41.5700 (3.72)      0.4210 (1.54)    1464;1691  25,611.2530 (0.29)      16767           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[128-4096]     10.6720 (1.02)     1,654.8910 (57.33)    1,480.0658 (128.98)     471.6188 (400.42)   1,632.0700 (145.98)    5.1940 (18.97)   1029;1059     675.6456 (0.01)      11000           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[512-512]      10.6810 (1.02)       186.9690 (6.48)       142.3081 (12.40)       40.9284 (34.75)      155.0470 (13.87)     0.5820 (2.13)    1107;1319   7,027.0076 (0.08)      12396           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[64-1024]      10.6820 (1.02)       860.0440 (29.79)       79.9685 (6.97)        21.5937 (18.33)       86.7600 (7.76)      0.7410 (2.71)    1408;1577  12,504.9224 (0.14)      13037           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[128-1024]     10.6950 (1.02)       151.0630 (5.23)       126.0578 (10.98)       34.1261 (28.97)      136.1520 (12.18)     0.7840 (2.86)    1119;1325   7,932.8702 (0.09)      13713           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[512-1024]     10.7210 (1.02)       657.4120 (22.77)      542.6207 (47.29)      168.3425 (142.93)     595.9800 (53.31)     0.6760 (2.47)    1042;1482   1,842.9080 (0.02)      11409           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[128-128]      10.7700 (1.03)        44.6940 (1.55)        11.6545 (1.02)         1.6967 (1.44)        11.3550 (1.02)      0.4252 (1.55)        43;64  85,803.7849 (0.98)        981           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[64-4096]      10.7720 (1.03)     1,022.5650 (35.42)      909.6811 (79.27)      281.8052 (239.26)     997.5710 (89.23)     1.6910 (6.18)    1034;2256   1,099.2863 (0.01)      11526           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[512-256]      10.8050 (1.03)        61.1240 (2.12)        38.0273 (3.31)         7.8161 (6.64)        40.3460 (3.61)      0.3280 (1.20)    1501;1731  26,296.8938 (0.30)      17761           1
test_attention_fp32_torch[128-256]                         19.0950 (1.82)        36.5030 (1.26)        20.2275 (1.76)         1.2942 (1.10)        19.5740 (1.75)      1.8500 (6.76)       449;59  49,437.6513 (0.57)       4834           1
test_attention_fp32_torch[128-128]                         19.1360 (1.82)        34.1990 (1.18)        19.9027 (1.73)         1.1778 (1.0)         19.6680 (1.76)      0.2738 (1.0)       234;319  50,244.4237 (0.58)       4387           1
test_attention_fp32_torch[64-1024]                         19.6310 (1.87)       241.4300 (8.36)       204.8426 (17.85)       42.3375 (35.95)      212.9265 (19.05)     1.9010 (6.94)     281;1032   4,881.7977 (0.06)       5646           1
test_attention_fp32_torch[512-128]                         20.5450 (1.96)        38.5030 (1.33)        24.8711 (2.17)         1.8885 (1.60)        25.5340 (2.28)      0.4233 (1.55)    1271;1435  40,207.3744 (0.46)       6393           1
test_attention_fp32_torch[128-1024]                        20.6400 (1.97)       268.8560 (9.31)       240.8562 (20.99)       42.5236 (36.10)      248.8490 (22.26)     1.1988 (4.38)      223;398   4,151.8556 (0.05)       6143           1
test_attention_fp32_torch[128-512]                         20.7700 (1.98)        92.6920 (3.21)        75.7514 (6.60)        10.6376 (9.03)        77.7610 (6.96)      0.9895 (3.61)      297;418  13,201.0730 (0.15)       7668           1
test_attention_fp32_torch[512-256]                         20.8890 (1.99)        90.2960 (3.13)        68.0172 (5.93)         9.3301 (7.92)        69.7665 (6.24)      0.8080 (2.95)      318;622  14,702.1710 (0.17)       7644           1
test_attention_fp32_torch[64-128]                          21.1540 (2.02)        96.5520 (3.34)        25.9655 (2.26)        17.1597 (14.57)       21.4280 (1.92)      0.6787 (2.48)          1;4  38,512.6800 (0.44)         19           1
test_attention_fp32_torch[64-256]                          21.2340 (2.02)       108.5550 (3.76)        23.9062 (2.08)         7.0356 (5.97)        22.4030 (2.00)      1.8590 (6.79)         6;15  41,830.2008 (0.48)        168           1
test_attention_fp32_torch[64-512]                          21.2680 (2.03)        91.2310 (3.16)        54.8779 (4.78)        12.2273 (10.38)       59.2750 (5.30)      0.8688 (3.17)      334;383  18,222.2571 (0.21)       2685           1
test_attention_fp32_torch[512-512]                         21.6970 (2.07)       171.7150 (5.95)       147.4023 (12.84)       21.0426 (17.87)      150.5295 (13.46)     1.3830 (5.05)      205;835   6,784.1527 (0.08)       7286           1
test_attention_fp32_torch[512-1024]                        22.1550 (2.11)       617.4150 (21.39)      589.6986 (51.39)       91.2108 (77.44)      604.6170 (54.08)     4.2065 (15.37)     177;250   1,695.7816 (0.02)       6975           1
test_attention_fp32_torch[128-4096]                        22.4930 (2.14)     3,464.3360 (120.01)   3,282.8247 (286.07)     576.4466 (489.42)   3,384.6690 (302.74)    8.1955 (29.94)     205;259     304.6157 (0.00)       6744           1
test_attention_fp32_torch[64-4096]                         22.9910 (2.19)     3,551.1790 (123.01)     201.6782 (17.57)      759.9540 (645.22)      23.9085 (2.14)      0.7320 (2.67)        11;47   4,958.3947 (0.06)        216           1
test_attention_fp32_torch[512-4096]                        23.0730 (2.20)     8,330.7730 (288.58)   7,480.1530 (651.84)   2,097.3720 (>1000.0)  8,053.3045 (720.33)   55.7670 (203.71)    204;224     133.6871 (0.00)       2784           1
test_attention_fp8_e5m2_acc_fp32_feather_gpu[64-128]       43.8750 (4.18)       116.3600 (4.03)        61.9504 (5.40)        30.6223 (26.00)       48.2500 (4.32)     22.8575 (83.50)         1;1  16,141.9457 (0.19)          5           1
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------